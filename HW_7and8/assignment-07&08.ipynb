{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-07&08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Using the new data to test different models\n",
    " + KNN, Logistic Regression, SVM, Bayes, Random Forest\n",
    " + To Classify if a new a by Xinhua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务描述\n",
    "报社等相关的机构，往往会遇到一个问题，就是别人家的机构使用自己的文章但是并没有标明来源。 在本次任务中，我们将解决新华社的文章被抄袭引用的问题。\n",
    "\n",
    "给定的数据集合中，存在一些新闻预料，该预料是来自新华社，但是其来源并不是新华社，请设计技巧学习模型解决该问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: 数据分析\n",
    "请在课程的GitHub中下载数据集，然后使用pandas进行读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/aistudio/data/data10734/sqlResult_1558435.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(filepath, encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: 数据预处理\n",
    "将pandas中的数据，依据是否是新华社的文章，请改变成新的数据dataframe: <content, y>, 其中，content是文章内容，y是0或者1. 你可能要使用到pandas的dataframe操作。https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2.1 先将语料中的文章进行jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting jieba\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/71/46/c6f9179f73b818d5827202ad1c4a94e371a29473b7f043b736b4dab6b8cd/jieba-0.39.zip (7.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.3MB 3.8MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Running setup.py bdist_wheel for jieba ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/aistudio/.cache/pip/wheels/46/2b/ce/f70aa4307a088454e032e45b63e6447d9ee70bcca85091704e\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.39\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    str_list = re.findall(r'[\\d|\\w]+', string)\n",
    "    pre_str = ''.join(str_list)\n",
    "    # print (temp_str)\n",
    "    if len(pre_str) == 0: pass\n",
    "    else: return ' '.join(jieba.cut(re.sub('n', '', pre_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.134 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "news_data['content'] = news_data['content'].fillna('').apply(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2.2 随机选一篇新闻查看分词后的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'受到 A股 被 纳入 MSCI 指数 的 利好 消息 刺激 A股 市场 从 周三 开始 再度 上演 龙马 行情 周四 上午 金融股 和 白马股 表现 喜人 但是 尾盘 跳水 之后 仅 金融 板块 仍 维系 红盘 状态 分析 人士 认为 金融股 受益 于 MSCI 纳入 A股 和 低 估值 而 重获 资金 青睐 但是 存量 资金 博弈 格局 下 风格 交替 的 震荡 格局 料 延续 流动性 改善 经济 悲观 预期 修正 等 有助于 支撑 板块 继而 大盘 指数 逐步 向 好 一九 再现 周四 A股 市场 未能 延续 周三 的 上行 态势 两市 成交 小幅 放量 29 个 中信 一级 行业 中 收盘 仅 银行 和 非银行 金融 两个 行业 指数 收红 分别 上涨 180 和 020 从 二级 行业 来看 股份制 与 城商行 的 涨幅 最高 达到 222 国有银行 上涨 082 信托 及其 他 上涨 064 保险 板块 上涨 034 证券 板块 上涨 006 银行 板块 25 只 成分股 中共 有 21 只 收红 其中 招商银行 涨幅 最大 上涨 666 贵阳 银行 上涨 365 上海银行 华夏银行 浦发银行 和 兴业银行 的 涨幅 均 超过 150 非银行 金融 板块 44 只 成分股 中共 17 只 个股 上涨 其中 安信 信托 中国 太保 涨幅 居前 两名 分别 上涨 457 和 304 西水股份 华安 证券 中国 人寿 和 新华 保险 的 涨幅 也 均 超过 2 相对而言 券商 股多 小幅 下跌 近期 对 A股 市场 消息面 影响 最大 的 就是 MSCI 宣布 从 2018 年 6 月 开始 将 A股 纳入 MSCI 新兴 市场 指数 而 其中 金融股 是 占 比 最大 的 一个 群体 国金 证券 李立峰 团队 指出 最新 方案 中 包含 的 222 只 成分股 中 剔除 了 中等 市值 非 互联互通 可 交易 的 股票 以及 有 停牌 限制 的 标的 由于 纳入 了 很多 大 市值 A H股 A股 在 MSCIEM 中 的 权重 由 05 上升 到 了 073 其中 金融 板块 占 比 最高 达到 4011 泛 消费 次之 占 比 为 2426 两个 板块 涵盖 了 大部分 权重股 动态 来看 由于 加入 了 很多 是 指 占 比高 的 金融公司 金融 板块 的 权重 增加 了 近一半 其他 大部分 行业 权重 都 受到 了 稀释 尽管 A股 被 纳入 MSCI 这一 利好 事件 对 短期 市场 情绪 有所 提振对 中长期 海外 增量 资金 预期 升温 但 短期内 市场 量 能 尚 不能 有效 放大 金融股 独 乐乐 情景 也 就 难以 持续 存量 博弈 格局 下 风格 交替 指数 震荡 格局 难 改变 光大 证券 指出 利好 并未 引起 市场 太大 的 热情 两市 指数 和 成交量 均 较为 平淡 但 市场 风格 出现 了 较大 变化 白马股 金融股 上涨 的 同时 成长 股 题材股 全天 低迷 这 表明 市场 增量 资金 依然 很少 存量 资金 在 不同 板块 之间 腾挪 这样 的 跷跷板 格局 使得 指数 难有 突破 市场 中期 依旧 偏空 短期 依旧 可能 维持 震荡 格局'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data['content'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2.3 将总样本（新闻语料）分为正样本（新华网新闻）和负样本（非新华网新闻），并将空的新闻语料剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_pos = news_data[news_data['source'].str.contains('新华')]\n",
    "news_data_pos = news_data_pos.fillna('')\n",
    "\n",
    "news_data_neg = news_data[news_data['source'].str.contains('新华') == 0]\n",
    "news_data_neg = news_data_neg.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78666\n",
      "8197\n"
     ]
    }
   ],
   "source": [
    "news_data_pos = news_data_pos[news_data_pos[\"content\"] != '']\n",
    "print (len(news_data_pos))\n",
    "news_data_neg = news_data_neg[news_data_neg[\"content\"] != '']\n",
    "print (len(news_data_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，正样本的数量为78666个，负样本的数量为8197个，为了使正负样本均衡，我们从正样本中随机选出8197个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8197\n",
      "8197\n"
     ]
    }
   ],
   "source": [
    "news_data_pos = news_data_pos.sample(n=len(news_data_neg))\n",
    "print (len(news_data_pos))\n",
    "print (len(news_data_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在正负样本的数量相同，已经很均衡了 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16394\n"
     ]
    }
   ],
   "source": [
    "news_data_sampled = news_data_pos.append(news_data_neg)\n",
    "print (len(news_data_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2.4 下面我们要为预处理好的文本内容打上标签了，新华网对应的标签为1，非新华网对应的标签为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_sampled['is_xinhua'] = np.where(news_data_sampled['source'].str.contains('新华'),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2.5 先选出一些数据作为test_data（占总样本的比例为10%）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "test_data = news_data_sampled.sample(n=int(16394*0.1))\n",
    "print (len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从news_data_sampled中去除掉test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_sampled = news_data_sampled.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18033"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_data_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = news_data_sampled.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'author', 'source', 'content', 'feature', 'title', 'url', 'is_xinhua']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_sampled = news_data_sampled.drop_duplicates(subset=columns,keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14755"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_data_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过上述步骤，我们成功将test_data从news_data_sampled中剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = news_data_sampled.loc[:,\"content\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs_test = test_data.loc[:,\"content\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs_total = np.append(x_inputs,x_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_inputs_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inputs = news_data_sampled['is_xinhua'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inputs_test = test_data['is_xinhua'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7372\n",
      "7383\n"
     ]
    }
   ],
   "source": [
    "print(len(news_data_sampled[news_data_sampled['source'] != '新华社']))\n",
    "print(len(news_data_sampled[news_data_sampled['source'] == '新华社']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14755"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: 使用tfidf进行文本向量化\n",
    "\n",
    "参考 https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html 对文本进行向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = TfidfVectorizer(max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_feature = vectorized.fit_transform(x_inputs_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16394, 10000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vecs_total = vec_feature.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vecs = np_vecs_total[0:14755,:]\n",
    "np_vecs_test = np_vecs_total[14755:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: 参考scikit-learning的方法，构建你的第一个机器学习模型\n",
    "+ 按照课程讲解的内容，把数据集分割为 traning_data, validation_data, test_data. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "+ 参照scikit learning的示例，从构建你的第一个KNN机器学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(\n",
    "    np_vecs , y_inputs, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clock(func):\n",
    "    def clocked(*args, **kwargs):\n",
    "        t0 = timeit.default_timer()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = timeit.default_timer() - t0\n",
    "        print(elapsed,'s')\n",
    "        return result\n",
    "    return clocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_performance(clf, x_, y_):\n",
    "    y_hat = clf.predict(x_)\n",
    "    print('f1_score is: {}'.format(f1_score(y_, y_hat)))\n",
    "    print('accuracy is: {}'.format(accuracy_score(y_, y_hat)))\n",
    "    print('percision is: {}'.format(precision_score(y_, y_hat)))\n",
    "    print('recall is: {}'.format(recall_score(y_, y_hat)))\n",
    "    print('roc_auc is: {}'.format(roc_auc_score(y_, y_hat)))\n",
    "    #print('confusion matrix: \\n{}'.format(confusion_matrix(y_, y_hat, labels=[0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN机器学习模型：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "@clock\n",
    "def KNN(x_train, y_train):\n",
    "    knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "    knn.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", knn.score(x_train, y_train))\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.8537783802100982\n",
      "341.2509812116623 s\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNN(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: 在traning_data, validation_data, test_data 上观察其相关metric: recall, precision, f1等， 并解释其含义. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 5.1 在traning_data上观察其相关metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.8316754437292764\n",
      "accuracy is: 0.8537783802100982\n",
      "percision is: 0.9886389983769998\n",
      "recall is: 0.7177242888402626\n",
      "roc_auc is: 0.8546833963389442\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model,x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 5.2 在validation_data上观察其相关metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.6845342706502635\n",
      "accuracy is: 0.756692646560488\n",
      "percision is: 0.9617283950617284\n",
      "recall is: 0.5313778990450204\n",
      "roc_auc is: 0.7552512390847999\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model,x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，训练出来的KNN模型在训练集上表现还算好，但是在验证集上的性能评估指标是真的差劲，综上可以初步猜测模型产生了过拟合。下面依次解释各性能评估指标的具体含义：\n",
    "+ accuracy 准确率：模型正确分类的数量占总样本数量的百分比；\n",
    "+ percision 精确率：在预测为正类的数据中，有多少预测正确；\n",
    "+ recall 召回率：在所有正类的数据中，有多少预测正确；\n",
    "+ f1_score F1值：percision和recall的调和均值；\n",
    "+ roc_auc：AUC的值为ROC曲线和坐标轴所围成的面积，越接近1表示模型的性能越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: 使用 test_data 对其进行新数据的判断, 你有没"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分我理解的意思是：在测试数据上观察模型表现的好坏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.6882591093117408\n",
      "accuracy is: 0.7651006711409396\n",
      "percision is: 0.9550561797752809\n",
      "recall is: 0.5379746835443038\n",
      "roc_auc is: 0.7572087787568398\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model, np_vecs_test, y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的性能指标中可以看出模型在测试数据上的表现很差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7: 调整不同的参数，观察变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一步中，我们将“n_neighbors”参数从3调整为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "@clock\n",
    "def KNN(x_train, y_train):\n",
    "    knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n",
    "    knn.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", knn.score(x_train, y_train))\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.8359030837004405\n",
      "281.647005751729 s\n"
     ]
    }
   ],
   "source": [
    "knn_model_1 = KNN(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 7.1 在traning_data上观察其相关metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.8052287581699347\n",
      "accuracy is: 0.8359030837004405\n",
      "percision is: 1.0\n",
      "recall is: 0.6739606126914661\n",
      "roc_auc is: 0.8369803063457331\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model_1,x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 7.2 在validation_data上观察其相关metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.667866786678668\n",
      "accuracy is: 0.7499152829549305\n",
      "percision is: 0.9814814814814815\n",
      "recall is: 0.5061391541609823\n",
      "roc_auc is: 0.7483557723666864\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model_1, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 7.3 在test_data上观察其相关metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.6727272727272727\n",
      "accuracy is: 0.7583892617449665\n",
      "percision is: 0.969047619047619\n",
      "recall is: 0.5151898734177215\n",
      "roc_auc is: 0.7499388707489079\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model_1, np_vecs_test, y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，将“n_neighbors”参数从3调整为2之后模型的性能并没有发生显著的提升，所以接下来使用scikit learning 中的GridSearchCV进行批量调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "@clock\n",
    "def KNN(x_train, y_train):\n",
    "    knn = KNeighborsClassifier()\n",
    "    param_grid = { 'n_neighbors' : [i for i in range(1,6)]}\n",
    "    clf = GridSearchCV(knn, param_grid, cv = 2, scoring='recall', n_jobs = -1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print('the optimal number of neighbors: ', clf.best_params_)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the optimal number of neighbors:  {'n_neighbors': 1}\n",
      "mean accuracy on the given test data and labels:  0.999663356337317\n",
      "1438.9814233165234 s\n"
     ]
    }
   ],
   "source": [
    "knn_model_2 = KNN(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，n_neighbors在区间[1,6]中选择1是最优的，我们接下来用该模型分别在训练、验证和测试集上观察性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.9998316498316498\n",
      "accuracy is: 0.9998305659098611\n",
      "percision is: 1.0\n",
      "recall is: 0.999663356337317\n",
      "roc_auc is: 0.9998316781686585\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model_2, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.7650058846606513\n",
      "accuracy is: 0.7970179600135547\n",
      "percision is: 0.9002770083102493\n",
      "recall is: 0.665075034106412\n",
      "roc_auc is: 0.7961738806895696\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model_2, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.7666422823701536\n",
      "accuracy is: 0.8053691275167785\n",
      "percision is: 0.9081455805892548\n",
      "recall is: 0.6632911392405063\n",
      "roc_auc is: 0.800432377629676\n"
     ]
    }
   ],
   "source": [
    "get_performance(knn_model_2, np_vecs_test, y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果中可以看出，n_neighbors=1时训练出来的模型总体效果要优于前两个模型（n_neighbors=2，n_neighbors=3）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step8: 不断改变参数，直到性能达到“某个”点。问：“某个”怎么定义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 我觉得“某个”的定义要根据具体的任务和要实现的效果来综合给出，像本次作业中训练出来的新华网文章判断模型，当性能指标accuracy在测试数据集上能够大于0.9，其它指标相对比较OK时，就可以认为训练出来的模型是一个比较好的模型了（能够基本完成所给定的任务），此时便可以终止调参这一步骤。\n",
    "+ 对于本次作业来说，当训练出来的模型accuracy>0.9时，其实已经比人类的判断要准确多了（我觉得大部分人类准确度达到70%就已经非常不错了），这时候的模型其实已经基本能够完成新华网文章判别任务了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step9: 找出所有预测为 1， 但是实际为 0 的文章。 作为抄袭的候选者。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = knn_model_2.predict(np_vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ = []\n",
    "for i in range(0, len(y_hat)):\n",
    "    if y_hat[i] == 1 and y_inputs_test[i] == 0:\n",
    "        index_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "plagiarized_candidate = x_inputs_test[index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'环球网 综合 报道 据 英国 每日 邮报 6 月 12 日 报道 在 葡萄牙 特 塞拉 岛 一名 中年男子 在 斗牛 现场 因 专心 录制 视频 未 发现 一头 狂暴 的 公牛 朝 自己 奔来 来不及 躲闪 被 顶 伤 目前 该 男子 伤势 尚 不 明确 从 视频 中 可以 看到 这位 名叫 达 玆 德 的 观众 正 忙于 用 平板 电脑 拍摄 斗牛 场面 未 注意 到 向 他 奔 去 的 公牛 公牛 先是 将 他 顶 向 空中 后 用 牛角 一直 砸击 达 玆 德 的 头部 和 胸部 达 玆 德 被 顶 向 空中 后 摔 到 水泥 地上 头部 受伤 直到 另 一位 观众 成功 转移 公牛 的 注意力 达 玆 德才 得以 获救 达 玆 德 松开 手时 伤口 有 血流 出特 塞拉 岛 的 居民 通常 在 每年 的 4 月 至 9 月 下旬 举行 斗牛 赛 今年 斗牛 赛 在 小镇 中心 举行 比赛 时 人们 稍稍 松开 公牛 的 绳子 不断 刺激 公牛 但 仍 由 8 人拉着 绳子 以免 公牛 失控 实习 编译 吴露鑫 审稿 朱盈库'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plagiarized_candidate[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plagiarized_candidate列表存储的就是从测试数据中找到的抄袭的候选者，一共有53篇文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step10： 总结该过程，什么是数据思维？什么是机器学习思维？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 数据思维：能够把需求或者业务问题转化为“数据可分析问题”，即能够找出自变量X和因变量Y。对数据进行合理的预处理，并将数据拟合为合适的“函数”。\n",
    "+ 机器学习思维：能够将“隐式的”机器学习问题转化为“显式的”机器学习问题，就像本次作业中的抄袭新华网文章判断问题，先训练一个能够准确判断新华网文章的模型，如果模型判断一篇文章是新华网的，但是文章的来源不是新华网，那么就可以认定这篇文章是抄袭新华网的，综上就是一个从“隐式”到“显式”的转化，即为机器学习思维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step11: 利用第8课讲述的新模型，进行操作，感受其中不同的参数、模型对性能的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Logistic Regression模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.9662826160623518\n",
      "16.96536340750754 s\n",
      "f1_score is: 0.9369608493696086\n",
      "accuracy is: 0.9420378279438683\n",
      "percision is: 0.9846582984658299\n",
      "recall is: 0.8936708860759494\n",
      "roc_auc is: 0.940357233379553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "@clock\n",
    "def logist(x_train, y_train):\n",
    "    clf = LogisticRegression(solver='saga').fit(x_train, y_train)\n",
    "    print (\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(logist(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **SVM模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.9982209420535412\n",
      "1.7159994263201952 s\n",
      "f1_score is: 0.9620578778135048\n",
      "accuracy is: 0.9640024405125076\n",
      "percision is: 0.9777777777777777\n",
      "recall is: 0.9468354430379747\n",
      "roc_auc is: 0.963405942955972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "@clock\n",
    "def SVM(x_train, y_train):\n",
    "    clf = LinearSVC(dual=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(SVM(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Bayes模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.9485767536428329\n",
      "6.862606760114431 s\n",
      "f1_score is: 0.8543177323665129\n",
      "accuracy is: 0.8651616839536302\n",
      "percision is: 0.8913342503438789\n",
      "recall is: 0.8202531645569621\n",
      "roc_auc is: 0.8636012583679981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "@clock\n",
    "def bayes(x_train, y_train):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(bayes(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.8924940698068451\n",
      "0.5890356041491032 s\n",
      "f1_score is: 0.8523489932885906\n",
      "accuracy is: 0.8657718120805369\n",
      "percision is: 0.9071428571428571\n",
      "recall is: 0.8037974683544303\n",
      "roc_auc is: 0.8636184043774506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "@clock\n",
    "def bayes_multi(x_train, y_train):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(bayes_multi(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.8773297187394103\n",
      "0.3489094488322735 s\n",
      "f1_score is: 0.8402061855670103\n",
      "accuracy is: 0.8486882245271506\n",
      "percision is: 0.8556430446194225\n",
      "recall is: 0.8253164556962025\n",
      "roc_auc is: 0.8478761312638844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "@clock\n",
    "def bayes_bernoulli(x_train, y_train):\n",
    "    clf = BernoulliNB(binarize=None)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(bayes_bernoulli(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Decision Tree模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.9998305659098611\n",
      "179.48635926656425 s\n",
      "f1_score is: 0.968025078369906\n",
      "accuracy is: 0.9688834655277608\n",
      "percision is: 0.9590062111801242\n",
      "recall is: 0.9772151898734177\n",
      "roc_auc is: 0.9691729659614436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "@clock\n",
    "def decision_tree(x_train, y_train):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(decision_tree(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Random Forest模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy on the given test data and labels:  0.9998305659098611\n",
      "88.22977033630013 s\n",
      "f1_score is: 0.9672025723472669\n",
      "accuracy is: 0.9688834655277608\n",
      "percision is: 0.9830065359477124\n",
      "recall is: 0.9518987341772152\n",
      "roc_auc is: 0.9682933011286546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "@clock\n",
    "def random_forest(x_train, y_train):\n",
    "    clf = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"mean accuracy on the given test data and labels: \", clf.score(x_train, y_train))\n",
    "    return clf\n",
    "\n",
    "get_performance(random_forest(x_train,y_train),np_vecs_test,y_inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结：从上面各模型的性能结果中可以看出，SVM、Decision Tree和Random Forest模型在本次作业任务中表现突出，其中Random Forest模型在测试数据集上的性能指标最好，但是SVM模型的训练时间最短且在测试数据集上的性能指标和Random Forest模型相差并不是很大。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary the advantages and disadvantages about Linear Regression, Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Bayes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=DeepPink size=3>由于这部分高老师上课没有细讲，看李航的《统计学习方法》也没有分析各个算法的优缺点，所以在网上看了一些博客，最后还是觉得这篇讲的比较详细：</font>\n",
    "+ 机器学习算法优缺点对比及选择（汇总篇） - 杜博亚的文章 - 知乎\n",
    "https://zhuanlan.zhihu.com/p/46831267"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上面这篇文章中有些地方还是不能太理解，等学习一段时间后自己再回过头来思考吧。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.5.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
