{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。\n",
    "\n",
    "<font color=DeepPink size=4>具体的复现内容详见同级目录中的“Lesson-01-HW.ipynb”</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```\n",
    "<font color=DeepPink size=4>以上问题答案已经邮件发送</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {<font color=DeepPink>自动驾驶、小度智能音箱、Google 翻译</font>}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans1: {<font color=DeepPink>首先在GitHub上注册账号并创建仓库，然后下载git并进行相关配置（用户名、邮箱和ssh key），最后即可在本地使用git命令操作仓库</font>}\n",
    "\n",
    "Ans2: {<font color=DeepPink>Jupyter：调试方便、实时显示、图表嵌入、可读性好（方便编写注释和文档）、易于展示和共享；</font>}\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;{<font color=DeepPink>Pycharm：能够提高python语言的开发效率（支持调试、语法高亮、Project管理、代码跳转、智能提示、单元测试）</font>}                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>概率模型是能够用概率分布表示的模型，即概率分布函数的集合</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>赌博、决策问题（下棋、机器人自主导航）、分类问题（判断图像类别、垃圾邮件分类）</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>因为相比于符号化编程，概率更加符合现实世界事物的变化规律。当规则较为简单时，使用基于解析和模式匹配的编程\n",
    "方法可以解决问题，但当规则很复杂时则很难编程实现；而且一旦规则和模式更改，代码也要随之更改，一点也不AI。但基于概率的方法可以很好的\n",
    "解决上述问题，eg.不论句子语法多么复杂，只需计算句子出现概率的大小便可以判断句子是否正确合理。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>能够反映句子S出现概率的模型即为（统计）语言模型；数学上讲就是句子S的概率分布P(S)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>机器翻译、拼写纠错、语音识别、自动文摘以及问答系统</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>$$ 将P(w_1w_2...w_m)=P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_m|w_1w_2...w_{m-1})简化为： $$</font>\n",
    "\n",
    "<font color=DeepPink>$$ P(w_1w_2...w_m)\\sim P(w_1)P(w_2)P(w_3)...P(w_m)即为{1-gram}语言模型 $$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink> 缺点：该模型假设一个词的出现与它周围的词是独立的，历史数据即约束信息少，导致辨别能力差，结果不准确</font>\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;<font color=DeepPink> 优点：训练语料库中出现的次数更多，具有更可靠的统计信息；模型简单，易于计算</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <font color=DeepPink>$$ 将P(w_1w_2...w_m)=P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_m|w_1w_2...w_{m-1})简化为： $$</font>\n",
    "\n",
    "<font color=DeepPink>$$ P(w_1w_2...w_m)\\sim P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_m|w_{m-1})即为{2-gram}语言模型 $$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b10000_10000&sec=1561818705&di=95ca9ff2ff37fcb88ae47b82c7079feb&src=http://s7.sinaimg.cn/mw690/006BKUGwzy75VK46FMi66&690)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eunuch = \"\"\"\n",
    "Eunuch = 敬语 日期 地点 助词 数词 名词 修饰词\n",
    "日期 = 前天 | 昨天 | 今天 | 明天 | 后天\n",
    "助词 = 里的\n",
    "数词 = 一个 | 一筐 | 一袋 | 一斤 | 一瓶\n",
    "名词 = 单个名词 | 名词 单个名词\n",
    "敬语 = 称谓 打招呼 | 称谓\n",
    "称谓 = 人称 ，\n",
    "人称 = 小主 | 娘娘 | 贵妃 | 皇上\n",
    "打招呼 = 吉祥！| 万岁！\n",
    "单个名词 = 柠檬 | 糖 | 汽水 | 香水 | 橘子\n",
    "修饰词 = 单个修饰词 | 修饰词 单个修饰词\n",
    "单个修饰词 = 很甜 | 很酸 | 很香 | 很漂亮 | 很好吃\n",
    "地点 = 皇宫 | 寝宫 | 御花园 | 御厨\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"\"\"\n",
    "location = 形容词 地名 动词 范围 方向\n",
    "地名 = 北京 | 上海 | 深圳 | 武汉 | 三亚 | 哈尔滨\n",
    "动词 = 在 | 位于 | 坐落在\n",
    "形容词 = 单个形容词 | 单个形容词 形容词\n",
    "单个形容词 = 繁华的 | 美丽的 | 寒冷的 | 炎热的 | 小资的\n",
    "方向 = 东面 | 西面 | 南面 | 北面\n",
    "范围 = 广东省的 | 黑龙江省的 | 湖北省的 | 河北省的 | 海南省的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(\"\\n\"):\n",
    "        if not line.strip(): continue\n",
    "        x, y = line.split(split)\n",
    "        grammar[x.strip()] = [s.split() for s in y.split(\"|\")]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_grammar = create_grammar(Eunuch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eunuch': [['敬语', '日期', '地点', '助词', '数词', '名词', '修饰词']],\n",
       " '日期': [['前天'], ['昨天'], ['今天'], ['明天'], ['后天']],\n",
       " '助词': [['里的']],\n",
       " '数词': [['一个'], ['一筐'], ['一袋'], ['一斤'], ['一瓶']],\n",
       " '名词': [['单个名词'], ['名词', '单个名词']],\n",
       " '敬语': [['称谓', '打招呼'], ['称谓']],\n",
       " '称谓': [['人称，']],\n",
       " '人称': [['小主'], ['娘娘'], ['贵妃'], ['皇上']],\n",
       " '打招呼': [['吉祥！'], ['万岁！']],\n",
       " '单个名词': [['柠檬'], ['糖'], ['汽水'], ['香水'], ['橘子']],\n",
       " '修饰词': [['单个修饰词'], ['修饰词', '单个修饰词']],\n",
       " '单个修饰词': [['很甜'], ['很酸'], ['很香'], ['很漂亮'], ['很好吃']],\n",
       " '地点': [['皇宫'], ['寝宫'], ['御花园'], ['御厨']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice as ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(gram, target):\n",
    "    if target not in gram: return target\n",
    "    expand = [generate(gram, t) for t in ch(gram[target])]\n",
    "    return \"\".join([i for i in expand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'娘娘，吉祥！后天御厨里的一瓶橘子很漂亮'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(create_grammar(Eunuch), \"Eunuch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(num):\n",
    "    for i in range(num):\n",
    "        print (generate(create_grammar(location), \"location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小资的深圳在河北省的南面\n",
      "寒冷的寒冷的小资的深圳在湖北省的西面\n",
      "小资的上海坐落在黑龙江省的西面\n",
      "小资的美丽的炎热的武汉位于广东省的西面\n",
      "小资的小资的北京在海南省的北面\n",
      "美丽的三亚位于河北省的东面\n",
      "繁华的哈尔滨位于广东省的南面\n",
      "繁华的武汉坐落在湖北省的西面\n",
      "寒冷的寒冷的美丽的北京在广东省的东面\n",
      "小资的炎热的三亚在海南省的东面\n"
     ]
    }
   ],
   "source": [
    "generate_n(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'E:/NLP/datasource/movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1 = 'E:/NLP/datasource/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(filename, encoding='utf_8', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content[\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'看枪版已足矣~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[80000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 进行文本清洗，获得所有的纯文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_1(string):\n",
    "    return re.findall(u'[\\u4e00-\\u9fa5]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'看枪版已足矣'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token(articles[80000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clean = [''.join(token(str(a))) for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/NLP/datasource/movie.txt', 'w', encoding='utf_8') as f:\n",
    "    for a in articles_clean:\n",
    "        f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/NLP/datasource/baoxian.txt', 'w', encoding='utf_8') as f:\n",
    "    for line in open(filename_1, encoding='utf_8'):\n",
    "        f.write(''.join(token_1(line)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 将这些文本进行切词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_1, TOKEN_2 = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\YJY\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.090 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(open('E:/NLP/datasource/movie.txt', encoding='utf_8')):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    \n",
    "    TOKEN_1 += cut(line.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(open('E:/NLP/datasource/baoxian.txt', encoding='utf_8')):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    \n",
    "    TOKEN_2 += cut(line.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = TOKEN_2 + TOKEN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4564621"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 331482),\n",
       " ('了', 102558),\n",
       " ('是', 75450),\n",
       " ('我', 52389),\n",
       " ('都', 36283),\n",
       " ('很', 34718),\n",
       " ('看', 34027),\n",
       " ('电影', 33675),\n",
       " ('在', 32158),\n",
       " ('也', 32066),\n",
       " ('和', 31441),\n",
       " ('有', 28712),\n",
       " ('不', 28547),\n",
       " ('就', 25689),\n",
       " ('人', 24006),\n",
       " ('好', 23026),\n",
       " ('啊', 20803),\n",
       " ('你', 18109),\n",
       " ('一个', 17563),\n",
       " ('这', 17490),\n",
       " ('还', 17454),\n",
       " ('还是', 16458),\n",
       " ('但', 15582),\n",
       " ('故事', 15010),\n",
       " ('没有', 14558),\n",
       " ('就是', 14007),\n",
       " ('喜欢', 13568),\n",
       " ('让', 13336),\n",
       " ('太', 12677),\n",
       " ('什么', 12248),\n",
       " ('又', 11566),\n",
       " ('剧情', 11359),\n",
       " ('没', 10858),\n",
       " ('说', 10774),\n",
       " ('吧', 10747),\n",
       " ('他', 10687),\n",
       " ('可以', 10676),\n",
       " ('不错', 10416),\n",
       " ('到', 10378),\n",
       " ('得', 10349),\n",
       " ('给', 10342),\n",
       " ('上', 10163),\n",
       " ('这个', 10059),\n",
       " ('被', 10046),\n",
       " ('对', 9921),\n",
       " ('能', 9703),\n",
       " ('最后', 9702),\n",
       " ('一部', 9693),\n",
       " ('片子', 9590),\n",
       " ('与', 9322)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33675"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count[\"电影\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    if words_count[word] == 0:\n",
    "        return 1 / len(TOKEN)\n",
    "    else:\n",
    "        return words_count[word] / len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007377392339911681"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1(\"电影\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['需要多长时间',\n",
       " '多长时间是否',\n",
       " '是否检查',\n",
       " '检查汽车保险',\n",
       " '汽车保险信贷',\n",
       " '信贷我',\n",
       " '我可以',\n",
       " '可以使用',\n",
       " '使用支付',\n",
       " '支付长期']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_2_GRAM[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的电影', 8640),\n",
       " ('看的', 7106),\n",
       " ('都是', 6335),\n",
       " ('让人', 5284),\n",
       " ('的故事', 4709),\n",
       " ('看了', 4585),\n",
       " ('也是', 4408),\n",
       " ('的时候', 4407),\n",
       " ('的人', 4376),\n",
       " ('的是', 4357),\n",
       " ('我的', 4213),\n",
       " ('看完', 3797),\n",
       " ('的片子', 3350),\n",
       " ('让我', 3283),\n",
       " ('这样的', 2854),\n",
       " ('这部电影', 2722),\n",
       " ('很好', 2647),\n",
       " ('电影的', 2551),\n",
       " ('不知道', 2540),\n",
       " ('的感觉', 2502),\n",
       " ('好的', 2482),\n",
       " ('自己的', 2369),\n",
       " ('了我', 2357),\n",
       " ('拍的', 2336),\n",
       " ('人的', 2309),\n",
       " ('中的', 2300),\n",
       " ('的一部', 2109),\n",
       " ('是我', 2089),\n",
       " ('是个', 2034),\n",
       " ('还不错', 1954),\n",
       " ('我觉得', 1927),\n",
       " ('喜欢的', 1888),\n",
       " ('这是', 1868),\n",
       " ('很有', 1850),\n",
       " ('不错的', 1817),\n",
       " ('他的', 1799),\n",
       " ('的我', 1754),\n",
       " ('都很', 1727),\n",
       " ('里的', 1724),\n",
       " ('最好的', 1709),\n",
       " ('的剧情', 1694),\n",
       " ('的演技', 1682),\n",
       " ('不喜欢', 1630),\n",
       " ('也很', 1606),\n",
       " ('的表演', 1589),\n",
       " ('你的', 1582),\n",
       " ('很喜欢', 1574),\n",
       " ('的很', 1566),\n",
       " ('了吧', 1529),\n",
       " ('好看的', 1505)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_2.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_2(word1, word2):\n",
    "    return (words_count_2[word1+word2] + 1) / (words_count[word1] + len(words_count_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    sentence_pro = prob_1(words[0])\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        \n",
    "        probability = prob_2(word, next_)\n",
    "        \n",
    "        sentence_pro *= probability\n",
    "    \n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.735208941410858e-25"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('洋葱奶昔来一杯')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.907491416275682e-20"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('养乐多绿来一杯')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5647526450237197e-42"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('小明今天抽奖抽到一台苹果手机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6080704433174245e-43"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('小明今天抽奖抽到一架波音飞机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上请你吃大餐，我们一起吃日料 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 1.510661782058605e-59\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 3.77803483300698e-60\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 5.675305871866843e-30\n",
      "---- 真是一只好看的小猫 with probility 2.1222781414002498e-24\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 2.7777301768843952e-21\n",
      "---- 今晚火锅去吃我 with probility 1.6931271879422976e-28\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 7.735208941410858e-25\n",
      "---- 养乐多绿来一杯 with probility 6.907491416275682e-20\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = get_probablity(s1), get_probablity(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(get_probablity, n = 10, gram = Eunuch, target = \"Eunuch\"):\n",
    "    \n",
    "    pro_list = [(get_probablity(generate(create_grammar(gram), target)), generate(create_grammar(gram), target)) for i in range(n)]\n",
    "    result_list = sorted(pro_list, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    for i in result_list:\n",
    "        print ('*'*2 + ' {} with probility {}'.format(i[1], i[0]) + '*'*2)\n",
    "    print ('\\n' + '-'*2 + ' {} is more possible '.format(result_list[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 皇上，今天寝宫里的一筐香水柠檬很酸 with probility 1.1085508257247917e-58**\n",
      "** 贵妃，吉祥！后天寝宫里的一个橘子汽水很酸 with probility 2.464256759862074e-59**\n",
      "** 娘娘，万岁！明天御厨里的一个汽水很酸 with probility 1.660628673860111e-64**\n",
      "** 皇上，吉祥！后天御花园里的一筐橘子糖很甜很甜很好吃 with probility 4.151282649481197e-65**\n",
      "** 小主，明天寝宫里的一袋橘子很漂亮 with probility 2.1322031599399507e-65**\n",
      "** 小主，吉祥！后天御花园里的一瓶香水很好吃 with probility 5.930347169917123e-66**\n",
      "** 娘娘，吉祥！明天皇宫里的一袋汽水很甜 with probility 1.9770388028376185e-66**\n",
      "** 贵妃，万岁！昨天御厨里的一斤柠檬橘子很漂亮很甜 with probility 1.976910043953082e-66**\n",
      "** 皇上，吉祥！前天御花园里的一斤香水香水很好吃很香 with probility 2.3954037587951715e-71**\n",
      "** 娘娘，明天御花园里的一个香水很香很香 with probility 1.7081087117310522e-71**\n",
      "** 贵妃，前天御花园里的一袋柠檬柠檬橘子很好吃很酸 with probility 7.767305170648621e-72**\n",
      "** 小主，明天御厨里的一瓶汽水香水很香很香 with probility 1.2154387885318576e-73**\n",
      "** 贵妃，后天御厨里的一瓶香水很香很甜很好吃 with probility 1.2465502535727207e-78**\n",
      "** 皇上，万岁！昨天寝宫里的一筐柠檬糖很香很香 with probility 1.1023001204795978e-83**\n",
      "** 娘娘，明天寝宫里的一斤橘子柠檬很漂亮 with probility 6.734742736771784e-85**\n",
      "** 娘娘，今天寝宫里的一个柠檬很好吃很酸很酸 with probility 1.0948749518174055e-86**\n",
      "** 娘娘，后天御厨里的一个柠檬很香 with probility 7.155895202644255e-87**\n",
      "** 皇上，吉祥！昨天御厨里的一筐糖香水香水很酸 with probility 1.3750758348190086e-90**\n",
      "** 小主，昨天皇宫里的一袋柠檬很香 with probility 1.5476418109361585e-91**\n",
      "** 娘娘，万岁！今天御花园里的一袋香水很漂亮很好吃很漂亮很酸 with probility 6.947501908813028e-96**\n",
      "\n",
      "-- 皇上，今天寝宫里的一筐香水柠檬很酸 is more possible \n"
     ]
    }
   ],
   "source": [
    "generate_best(get_probablity, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 炎热的美丽的北京坐落在黑龙江省的南面 with probility 8.434682880002155e-38**\n",
      "** 小资的寒冷的小资的北京在海南省的南面 with probility 1.6872652564105416e-38**\n",
      "** 繁华的炎热的寒冷的上海坐落在黑龙江省的南面 with probility 5.0619805170916925e-39**\n",
      "** 炎热的上海位于海南省的西面 with probility 1.5517207304229362e-40**\n",
      "** 美丽的小资的哈尔滨坐落在海南省的西面 with probility 1.1218069345155769e-40**\n",
      "** 小资的深圳位于广东省的西面 with probility 5.609144905239736e-41**\n",
      "** 炎热的炎热的深圳位于河北省的东面 with probility 9.312420752494601e-42**\n",
      "** 寒冷的武汉在河北省的东面 with probility 9.205055562365476e-43**\n",
      "** 炎热的繁华的寒冷的北京坐落在海南省的北面 with probility 9.496019455335466e-46**\n",
      "** 美丽的小资的炎热的炎热的三亚位于黑龙江省的西面 with probility 2.5773634648262153e-47**\n",
      "** 美丽的美丽的美丽的寒冷的小资的小资的美丽的炎热的小资的寒冷的小资的哈尔滨在海南省的东面 with probility 1.5505871697026337e-47**\n",
      "** 美丽的武汉在湖北省的西面 with probility 6.299331117869187e-48**\n",
      "** 炎热的小资的深圳坐落在海南省的南面 with probility 1.582028348919778e-48**\n",
      "** 寒冷的美丽的繁华的哈尔滨在广东省的西面 with probility 1.1506610922947125e-49**\n",
      "** 小资的武汉位于湖北省的南面 with probility 8.786424883356047e-52**\n",
      "** 繁华的炎热的炎热的繁华的哈尔滨位于湖北省的西面 with probility 6.188385183898798e-52**\n",
      "** 炎热的北京在广东省的东面 with probility 1.4910137234424628e-52**\n",
      "** 寒冷的小资的美丽的寒冷的武汉在河北省的南面 with probility 2.5635656032038744e-53**\n",
      "** 小资的北京位于河北省的东面 with probility 9.726188355502197e-54**\n",
      "** 寒冷的上海坐落在广东省的南面 with probility 1.4283428031239107e-88**\n",
      "\n",
      "-- 炎热的美丽的北京坐落在黑龙江省的南面 is more possible \n"
     ]
    }
   ],
   "source": [
    "generate_best(get_probablity, 20, location, \"location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: &ensp;<font color=DeepPink>按照课上老师的方法，generate_best函数生成的句子中的有些分词可能没有在训练语料库中出现，所以会导致最后计算出句子出现的概率为0，为了解决这个问题，本次作业采用了拉普拉斯平滑方法，避免了句子概率为0情况的发生。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢\n",
    "\n",
    "<font color=DeepPink size=4>阿兰图灵机器智能原始论文的阅读已经邮件发送</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
